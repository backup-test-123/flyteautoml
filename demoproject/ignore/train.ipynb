{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import keras\n",
    "import ujson\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from models.classifier.resnet50.constants import CHECKPOINT_FILE_NAME\n",
    "from models.classifier.resnet50.constants import FINAL_FILE_NAME\n",
    "\n",
    "from flytekit.contrib.notebook import python_notebook\n",
    "from flytekit.sdk.tasks import inputs, outputs\n",
    "from flytekit.sdk.types import Types\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def print_dir(directory, logger):\n",
    "    for r, d, files in os.walk(directory):\n",
    "        logger.info(r, d, files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Training function\n",
    "def train_resnet50_model(\n",
    "    train_directory,\n",
    "    validation_directory,\n",
    "    output_model_folder,\n",
    "    logger,\n",
    "    patience,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    size,\n",
    "    weights,\n",
    "):\n",
    "    logger.info(\n",
    "        f\"Train Resnet 50 called with Train: {train_directory}, Validation: {validation_directory}\"\n",
    "    )\n",
    "    print_dir(train_directory, logger)\n",
    "    print_dir(validation_directory, logger)\n",
    "\n",
    "    # Creating a data generator for training data\n",
    "    gen = keras.preprocessing.image.ImageDataGenerator()\n",
    "\n",
    "    # Creating a data generator and configuring online data augmentation for validation data\n",
    "    val_gen = keras.preprocessing.image.ImageDataGenerator(\n",
    "        horizontal_flip=True, vertical_flip=True\n",
    "    )\n",
    "\n",
    "    # Organizing the training images into batches\n",
    "    batches = gen.flow_from_directory(\n",
    "        train_directory,\n",
    "        target_size=size,\n",
    "        class_mode=\"categorical\",\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    num_train_steps = len(batches)\n",
    "    if not num_train_steps:\n",
    "        raise Exception(\"No training batches\")\n",
    "    logger.info(\"num_train_steps = %s\" % num_train_steps)\n",
    "\n",
    "    # Organizing the validation images into batches\n",
    "    val_batches = val_gen.flow_from_directory(\n",
    "        validation_directory,\n",
    "        target_size=size,\n",
    "        class_mode=\"categorical\",\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    num_valid_steps = len(val_batches)\n",
    "    if not num_valid_steps:\n",
    "        raise Exception(\"No validation batches.\")\n",
    "    logger.info(\"num_valid_steps = %s\" % num_valid_steps)\n",
    "\n",
    "    # Picking the predefined ResNet50 as our model, and initialize it with a weight file\n",
    "    model = keras.applications.resnet50.ResNet50(weights=weights)\n",
    "\n",
    "    # Change resnet from a binary classifier to a multi-class classifier by removing the last later\n",
    "    classes = list(iter(batches.class_indices))\n",
    "    model.layers.pop()\n",
    "\n",
    "    # Since we don't have much training data, we want to leverage the feature learned from a larger dataset, in this,\n",
    "    # case, imagenet. So we fine-tune based on a pre-trained weight by freezing the weights except for the last layer\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Attaching a fully-connected layer with softmax activation as the last layer to support multi-class classification\n",
    "    last = model.layers[-1].output\n",
    "    x = Dense(len(classes), activation=\"softmax\")(last)\n",
    "\n",
    "    finetuned_model = Model(inputs=model.input, outputs=x)\n",
    "\n",
    "    # Compile the model with an optimizer, a loss function, and a list of metrics of choice\n",
    "    finetuned_model.compile(\n",
    "        optimizer=Adam(lr=0.00001),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    for c in batches.class_indices:\n",
    "        classes[batches.class_indices[c]] = c\n",
    "    finetuned_model.classes = classes\n",
    "\n",
    "    # Setting early stopping thresholds to reduce training time\n",
    "    early_stopping = EarlyStopping(patience=patience)\n",
    "\n",
    "    # Checkpoint the current best model\n",
    "    checkpointer = ModelCheckpoint(\n",
    "        output_model_folder + \"/\" + CHECKPOINT_FILE_NAME, verbose=1, save_best_only=True\n",
    "    )\n",
    "\n",
    "    # Train it\n",
    "    finetuned_model.fit_generator(\n",
    "        batches,\n",
    "        steps_per_epoch=num_train_steps,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping, checkpointer],\n",
    "        validation_data=val_batches,\n",
    "        validation_steps=num_valid_steps,\n",
    "    )\n",
    "\n",
    "    finetuned_model.save(output_model_folder + \"/\" + FINAL_FILE_NAME)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# The main training notebook task\n",
    "def train_on_datasets(\n",
    "    wf_params,\n",
    "    train_zips,\n",
    "    validation_zips,\n",
    "    model_config_string,\n",
    "    model_output_path,\n",
    "    model_blobs,\n",
    "    model_files_names,\n",
    "):\n",
    "    metadata_folder = wf_params.working_directory.get_named_tempfile(\"metadata\")\n",
    "    Path(metadata_folder).mkdir(0o777, parents=True, exist_ok=False)\n",
    "\n",
    "    zips_folder = wf_params.working_directory.get_named_tempfile(\"zips\")\n",
    "    Path(zips_folder).mkdir(0o777, parents=True, exist_ok=False)\n",
    "\n",
    "    train_dataset = wf_params.working_directory.get_named_tempfile(\"train\")\n",
    "    Path(train_dataset).mkdir(0o777, parents=True, exist_ok=False)\n",
    "\n",
    "    validation_dataset = wf_params.working_directory.get_named_tempfile(\"validate\")\n",
    "    Path(validation_dataset).mkdir(0o777, parents=True, exist_ok=False)\n",
    "\n",
    "    output_folder = wf_params.working_directory.get_named_tempfile(\"output\")\n",
    "    Path(output_folder).mkdir(0o777, parents=True, exist_ok=False)\n",
    "\n",
    "    model_config = ujson.loads(model_config_string)\n",
    "    train_streams = flatten_session_sub_path_stream_tuple(\n",
    "        model_config.get(\"train_datasets\", {})\n",
    "    )\n",
    "    validaton_streams = flatten_session_sub_path_stream_tuple(\n",
    "        model_config.get(\"validation_datasets\", {})\n",
    "    )\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    download_and_arrange_datasets_for_resnet(\n",
    "        data_streams=train_streams,\n",
    "        zips_folder=zips_folder,\n",
    "        blobs=train_zips,\n",
    "        out_path=train_dataset,\n",
    "        s3_client=s3_client,\n",
    "        tmp_metadata_folder=metadata_folder,\n",
    "    )\n",
    "    download_and_arrange_datasets_for_resnet(\n",
    "        data_streams=validaton_streams,\n",
    "        zips_folder=zips_folder,\n",
    "        blobs=validation_zips,\n",
    "        out_path=validation_dataset,\n",
    "        s3_client=s3_client,\n",
    "        tmp_metadata_folder=metadata_folder,\n",
    "    )\n",
    "\n",
    "    # TODO: read overrides for some of these values from the model_config.json\n",
    "    train_resnet50_model(\n",
    "        train_dataset,\n",
    "        validation_dataset,\n",
    "        output_folder,\n",
    "        logger=wf_params.logging,\n",
    "        patience=DEFAULT_PATIENCE,\n",
    "        size=DEFAULT_IMG_SIZE,\n",
    "        batch_size=DEFAULT_BATCH_SIZE,\n",
    "        epochs=DEFAULT_EPOCHS,\n",
    "        weights=DEFAULT_WEIGHTS,\n",
    "    )\n",
    "\n",
    "    # save results to Workflow output\n",
    "    blobs, files_names_list = blobs_from_folder_recursive(output_folder)\n",
    "    model_blobs.set(blobs)\n",
    "    model_files_names.set(files_names_list)\n",
    "\n",
    "    \"\"\"\n",
    "    # write results to storage path also\n",
    "    for file in files_names_list:\n",
    "        location = model_output_path + file\n",
    "        out_blob = Types.Blob.create_at_known_location(location)\n",
    "\n",
    "        with out_blob as out_writer:\n",
    "            with open(output_folder + \"/\" + file, mode=\"rb\") as in_reader:\n",
    "                out_writer.write(in_reader.read())\n",
    "\n",
    "    # keep the model_config with the trained model\n",
    "    location = model_output_path + MODEL_CONFIG_FILE_NAME\n",
    "    out_blob = Types.Blob.create_at_known_location(location)\n",
    "    with out_blob as out_writer:\n",
    "        out_writer.write((model_config_string).encode(\"utf-8\"))\n",
    "\n",
    "    # write metadata to track what execution this was done by\n",
    "    location = model_output_path + MODEL_GENERATED_BY_FILE_NAME\n",
    "    out_blob = Types.Blob.create_at_known_location(location)\n",
    "    with out_blob as out_writer:\n",
    "        out_writer.write((f\"workflow_id: {wf_params.execution_id}\").encode(\"utf-8\"))\n",
    "    \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_blobs, model_files_names = train_on_datasets()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from flytekit.contrib.notebook import record_outputs\n",
    "record_outputs({\n",
    "    \"model_blobs\": model_blobs,\n",
    "    \"model_files_names\": model_files_names,\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}